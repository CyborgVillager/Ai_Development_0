       Key Legend:
                -- = current version
                -wor: = working on this ver
                 <-x_x-> = bug
                 |x_x| = bug fixed
                 <-o_o-> = design issue
                 |o_o| = design issue fixed


                     ~Updating to repo~
               | --------------------------- |
               | v# : Short Version Title    |
               | Description: v# description |
               | --------------------------- |


v0 Intro to Ai Dev Everything has it's Beginning: Will be completing a video from Tech With Tim in learning the
                                                    different areas of Machine learning/Ai
                                                    His Desc:
                                                    Ready to explore machine learning and artificial intelligence in python?
                                                    This python machine learning and AI mega course contains 4 different
                                                    series designed to teach you the ins and outs of ML and AI.
                                                    It talks about fundamental ML algorithms, neural networks, creating
                                                    AI chat bots and finally developing an AI that can play the game of Flappy Bird.
                                           Video Link: https://www.youtube.com/watch?v=WFr2WgN9_xE

v1 Rebooted: Rebooted orig files, starting from scratch and build up from there.

v2 Installed: Installed tensorflow & Keras
                v2.5 Edit: Paused will resume @ 1/28/2020

v3 Pip install: installed sklearn, panda, & numpy, made a folder to hold .csv files. test.py now prints out the
                    csv files. Will work on making the program predict on 1/29/20
----------------------------    ----------------------------   ------------------------- ------------------------------
v4 Beta version .5 Prediction Implemented: The program will now train it self 4 90% & do test 4 10%
                                Line 22 @ test.py takes all of the attributes from Features & Labels ->
                                the program will split them up into 4 arrays  x_train -> section of numpy.array
                                (data.drop([predictiction],1)) , y_train -> numpy.array(data[predictiction])
                                 x_test & y_test will test the accuracy of the program/model.

                                 Imported import sklearn to session1_source/tim_lesson_source.py
                                 Program has basic prediction, of the excel file student-mat.csv, will upgrade further

v5 Prediction Assembled .8: Program test.py can now predict student's grade within a margin,
                                further work is needed to improve accuracy, will test the array from the models that
                                 were not trained on and see the results from there.

                                 Updated ReadMe.md, learned a bit more about ReadMe.txt posting for version control
                                 will be using # to make a title -> # Ai_Development_0
----------------------------    ----------------------------   ------------------------- ------------------------------
v6 matplotlip Installed:  open / create a file called studentmodels and save it by using pickle.dump in the directory
                              link for pickle https://docs.python.org/3/library/pickle.html
                                Side note for future reference pickle is not secure, so for future
                                files make sure its clean

v7 Process Increase: Commented out line 26-40 @  file test.py, now it  loads/open & start @
                    pickle_in = open('studentmodels.pickle', 'rb')  the program skips the training process and heads
                    straight to getting the data & saving it aka processing the data is faster

v8  Re-Training Model for the students Modified: line 22-42 has been commented out for re-training the program,
                                                    moved x_train, x_test, y_train, y_test =
                                                    sklearn.model_selection.train_test_split(X, Y, test_size = 0.1)
                                                    to line 19.

v9 Graph Now Shows Data: Line 73 - 85 now will aquire the information and displays it onto a graph for
                            visual data gathering, and helps finding correlation between various subjects
----------------------------    ----------------------------   ------------------------- -----------------------------
v10 Introduction to K-Nearest Neighbors: New folder called Introduction to KNN has been made.
                                            Basic info from the readme that has been created:
                                            This folder will be a mla program to gather/classify data. This machine
                                             learning algorithm will be classifing the data into each of its own
                                             categories that it will fit.

                                             Example: if the data is a dog it will be classified as a canine, cat ->
                                             feline, tortiose -> reptile, etc.

                                             For this folder it will be classifying cars in four categories depending on
                                             the car features.

                                             Source folder has been created called knn source to be used on .py file
                                             knn_tutorial. The car data has been imported to car_data_set for this
                                             experiment

v11 README Introduction to K-Nearest Neighbors Updated: Updated the information to Introduction to K-Nearest Neighbors
                                                            to assist in knowing a bit more about the application

v12 Printed Out-Statement: knn_tutorial is now able to print the results from car_data_set/car.data

v13 Basic Results: Results from car.data can be organized/displayed regarding their category @ file knn_tutorial.py
                    more work is needed to refine, will update by the end of 1/31/20

v14 Splitting Test: Program can split/train it self and see the possibilities from the data that has been provided
----------------------------    ----------------------------   ------------------------- -----------------------------
v15 K-Nearest Neighbors: " K-Nearest Neighbors works by looking at the K closest points to the given data point
                            (the one we want to classify) and picking the class that occurs the most to be the predicted value"
                            ~ Tech With Tim

                            K = means hyper parameter / stands for the amount of neighbors the program will look for
                            if K = 3, the program will look for 3 closest neighbors around the point The points will 'vote'
                             and classify it.


v16 Attributes Can Now Be Shown: Attributes for the car data can now be seen by the user once the program knn_tutorial.py
                                    runs. Will show the car condition, prediction and the data involved for the car information


v17 Scikit-learn / Un-organized Info: Info using scikit is able to be seen by user, however as of now
                                      the display is dis-organzied. Will work on fixing it
                                      Link for more info for scikit: https://scikit-learn.org/stable/


v18 SVM Tutorial : 'SVM stands for a support vector machine. SVM's are typically used for classification tasks similar
                        to what we did with K Nearest Neighbors. They work very well for high dimensional data and
                        are allow for us to classify data that does not have a linear correspondence.'
                        ~ Tim

                        New file -> svm_tutorial.py & source file -> svm_source.
                        Changed Introduction to KNN to Introduction to MLA.
                        Main objective for the file svm_tutorial is to see the breast cancer data set and check if the
                        classification can see if its tumor/cancer/non-cancer .

                        Edit: https://scikit-learn.org/stable/


v19 Learning On How Support Vector Machine Works: ' support vector machine works by dividing data into multiple classes
                                                    using something called a hyper-plane. A hyper plane is a fancy word
                                                    for something that is straight that can divide data points. In 2D space
                                                     a hyper-plane is simply a line, in 3D space a hyper-plane is a plane.
                                                      In any space higher than 3D it is simply called a hyper-plane.'
                                                      ~ Tim
                                                      Link: https://techwithtim.net/tutorials/machine-learning-python/svm-2/
                                                          The larger the distance / easier to seperate the points and classify
                                                      it at a better prediction on what it is.

                                                     Kernels:
                                                     'Kernels provide a way for us to create a hyper-plane for data like seen above.
                                                      We use a kernel to bring our data up to a higher dimension (in this case from 2D->3D).
                                                      We hope that by doing this we will have our points plotted in a way that we
                                                      can divide them using a hyper-plane.'

                                                      ~ Tim
                                                      Function for kernel f(x1,x2) -> x3 or f(x1,x2) = x3


----------------------------    ----------------------------   ------------------------- -----------------------------
v20 KNeighborsClassifier: line 21 -> clf = KNeighborsClassifier(n_neighbors=15) used to find nearest neighbors

v21 Unsupervised Algorithm K Means Clustering: Aquired K Means Clustring example from Scikit ->
                                                   Intro To K Means Clustering/scikit_example_kmeans.py

                                                   Link to Tim's quote on K-Means
                                                   https://techwithtim.net/tutorials/machine-learning-python/k-means-1/
                                                   K-Means Clustering
'K Means clustering is an unsupervised learning algorithm that attempts to divide our training data into k unique clusters to
classify information. This means this algorithm does not require labels for given test data. It is responsible for learning the
differences between our data points and determine what features determining what class.

Supervised vs Unsupervised Algorithm
Up until this point we have been using supervised machine learning algorithms. Supervised learning means that when we
pass training data to our algorithm we also pass the estimated values or classes for each of those data points. For
example, when we were classifying the safety of cars we gave the algorithm the features of the car and we told it if
the car was safe or not. If we were using an unsupervised algorithm we would only pass the features and omit the class.

How K-Means Clustering Works
The K-Means clustering algorithm is a classification algorithm that follows the steps outlined below to
cluster data points together. It attempts to separate each area of our high dimensional space into sections that
represent each class. When we are using it to predict it will simply find what section our point is in and assign it to
 that class.

Step 1: Randomly pick K points to place K centroids
Step 2: Assign all of the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.
Step 3: Average all of the points belonging to each centroid to find the middle of those clusters (center of mass).
Place the corresponding centroids into that position.
Step 4: Reassign every point once again to the closest centroid.
Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.'
----------------------------    ----------------------------   ------------------------- -----------------------------
v22 K Means: New file created k_means.py with source folder containing both scikit example & k means source info.
               link for more info -> https://techwithtim.net/tutorials/machine-learning-python/k-means-2/
                 https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
                 https://scikit-learn.org/stable/modules/clustering.html
                 https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation


v23 TensorFlow & File Moves: Moved Knn, Svm, & Kmeans files to a folder called KNN_SVM_and_Kmeans
                                to better organize for the lesson, and to make a new folder called
                                 TensorFlow for the upcoming lessons.

v24 Installed Tensorflow: installed Tensorflow ->
        Link : https://www.tensorflow.org/tutorials/keras/classification
        file tensorflow0.py can now show images of the clothing objects
        v24.5 Edit: cmap=plt.cm.binary was added to the clothing object now show its 'true colors'
        v24.8 Compression of Data: compress the data to 28 by 28 pixels. line 11-12


v25 Flatting the Data & Neural Network Setup: Compress the data, easier for the system to read/process for itself
                                                 the file tensorflow0.py now has a basic Neural Network to check the
                                                 images and make and accuracte guess to what it is



v26 Training the Model: line 39-42  Training the model ->
                                    prediction = model.predict(test_images)
                                     argmax -> gets the largest value and gets the largest index of that item
                                        print(class_names[np.argmax(prediction[0])])

                                     The program will find the model/predict it after that it will find the largest
                                     value then go to the largest index of that item. Once that happens it can print out
                                     a class name with the prediction value


v27 Prediction Validation: the program can now test/train it self for the clothing images, then show
                                to the user the results on SciView. The graph shows the clothing
                                name, and have the 'results on actual' that tells the user the actual
                                item name

----------------------------    ----------------------------   ------------------------- -----------------------------

v28 Temp Suspend: Will resume this project on 2/5/20

v29 Resume 2/6/20: Updated TensorFlow folder to include tensorflow1 for text classification

v30 Ai_Development_1 Created: Created a new folder to hold future information regarding A.I development. Updated
                                tensorflow1.py to print out a list from keras.datasets.imdb.
                                Each of these int points to certain words, helps the user to find specific information
                                of their choosing by making the movie review into a list, now need to work on a mapping
                                to make it easier for humans to read it.


v31 Updated Folder Placement: Updated folder placement for Machine_Learning_0, & Ai_Development_0. Due to .git not
                                    be able to see my custom folder/information on v30.


v31 tensorflow1 <START>: Information can be aquired / decoded into human language @ tensorflow1.py
                            Commented/info from tensorflow1.py
                            -> training data set which are associated  with the keys
                            -> as of now just adding 3 from line 16 to make our own values that stands for line 20-23
                            -> PAD = padding, Unk = unknown
                            -> swap the values in the keys, which makes the dict to have the value 1st as in int goes to the word
                            -> similiar to  print(train_data[0]) but for humans @ line 10.
                            -> basically just reversing the info dict
                            -> reverse_word_index = dict([(value,key) for (key,value) in words_index.items()])

                            line 33 decodes the information into human language.
                            line 41-43 Results
----------------------------    ----------------------------   ------------------------- -----------------------------

v31 Pre-processing the data, makes the form consistent: Also added Defining the model information line 51-57 to
                                                            check whether or not the movie is good or bad, need to work
                                                            further to show results.
                                                            v31.5 Embeded Layer Info: Embedding Layers to make
                                                                                similiar information closer together


v32 Basic Prediction / model Review: the program can do some prediction of a review for a commenter response to a
                                        movie.


--v33 Saving & Loading Data: Also known as check-pointing the model - > predicition is able to aquire data from saved
                                data then can process it a bit faster by cutting off un-ness info. line 104-114 is used
                                  for model prediction