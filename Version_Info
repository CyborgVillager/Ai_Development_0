       Key Legend:
                -- = current version
                -wor: = working on this ver
                 <-x_x-> = bug
                 |x_x| = bug fixed
                 <-o_o-> = design issue
                 |o_o| = design issue fixed


                     ~Updating to repo~
               | --------------------------- |
               | v# : Short Version Title    |
               | Description: v# description |
               | --------------------------- |


v0 Intro to Ai Dev Everything has it's Beginning: Will be completing a video from Tech With Tim in learning the
                                                    different areas of Machine learning/Ai
                                                    His Desc:
                                                    Ready to explore machine learning and artificial intelligence in python?
                                                    This python machine learning and AI mega course contains 4 different
                                                    series designed to teach you the ins and outs of ML and AI.
                                                    It talks about fundamental ML algorithms, neural networks, creating
                                                    AI chat bots and finally developing an AI that can play the game of Flappy Bird.
                                           Video Link: https://www.youtube.com/watch?v=WFr2WgN9_xE

v1 Rebooted: Rebooted orig files, starting from scratch and build up from there.

v2 Installed: Installed tensorflow & Keras
                v2.5 Edit: Paused will resume @ 1/28/2020

v3 Pip install: installed sklearn, panda, & numpy, made a folder to hold .csv files. test.py now prints out the
                    csv files. Will work on making the program predict on 1/29/20
----------------------------    ----------------------------   ------------------------- ------------------------------
v4 Beta version .5 Prediction Implemented: The program will now train it self 4 90% & do test 4 10%
                                Line 22 @ test.py takes all of the attributes from Features & Labels ->
                                the program will split them up into 4 arrays  x_train -> section of numpy.array
                                (data.drop([predictiction],1)) , y_train -> numpy.array(data[predictiction])
                                 x_test & y_test will test the accuracy of the program/model.

                                 Imported import sklearn to session1_source/tim_lesson_source.py
                                 Program has basic prediction, of the excel file student-mat.csv, will upgrade further

v5 Prediction Assembled .8: Program test.py can now predict student's grade within a margin,
                                further work is needed to improve accuracy, will test the array from the models that
                                 were not trained on and see the results from there.

                                 Updated ReadMe.md, learned a bit more about ReadMe.txt posting for version control
                                 will be using # to make a title -> # Ai_Development_0
----------------------------    ----------------------------   ------------------------- ------------------------------
v6 matplotlip Installed:  open / create a file called studentmodels and save it by using pickle.dump in the directory
                              link for pickle https://docs.python.org/3/library/pickle.html
                                Side note for future reference pickle is not secure, so for future
                                files make sure its clean

v7 Process Increase: Commented out line 26-40 @  file test.py, now it  loads/open & start @
                    pickle_in = open('studentmodels.pickle', 'rb')  the program skips the training process and heads
                    straight to getting the data & saving it aka processing the data is faster

v8  Re-Training Model for the students Modified: line 22-42 has been commented out for re-training the program,
                                                    moved x_train, x_test, y_train, y_test =
                                                    sklearn.model_selection.train_test_split(X, Y, test_size = 0.1)
                                                    to line 19.

v9 Graph Now Shows Data: Line 73 - 85 now will aquire the information and displays it onto a graph for
                            visual data gathering, and helps finding correlation between various subjects
----------------------------    ----------------------------   ------------------------- -----------------------------
v10 Introduction to K-Nearest Neighbors: New folder called Introduction to KNN has been made.
                                            Basic info from the readme that has been created:
                                            This folder will be a mla program to gather/classify data. This machine
                                             learning algorithm will be classifing the data into each of its own
                                             categories that it will fit.

                                             Example: if the data is a dog it will be classified as a canine, cat ->
                                             feline, tortiose -> reptile, etc.

                                             For this folder it will be classifying cars in four categories depending on
                                             the car features.

                                             Source folder has been created called knn source to be used on .py file
                                             knn_tutorial. The car data has been imported to car_data_set for this
                                             experiment

v11 README Introduction to K-Nearest Neighbors Updated: Updated the information to Introduction to K-Nearest Neighbors
                                                            to assist in knowing a bit more about the application

v12 Printed Out-Statement: knn_tutorial is now able to print the results from car_data_set/car.data

v13 Basic Results: Results from car.data can be organized/displayed regarding their category @ file knn_tutorial.py
                    more work is needed to refine, will update by the end of 1/31/20

v14 Splitting Test: Program can split/train it self and see the possibilities from the data that has been provided
----------------------------    ----------------------------   ------------------------- -----------------------------
v15 K-Nearest Neighbors: " K-Nearest Neighbors works by looking at the K closest points to the given data point
                            (the one we want to classify) and picking the class that occurs the most to be the predicted value"
                            ~ Tech With Tim

                            K = means hyper parameter / stands for the amount of neighbors the program will look for
                            if K = 3, the program will look for 3 closest neighbors around the point The points will 'vote'
                             and classify it.


v16 Attributes Can Now Be Shown: Attributes for the car data can now be seen by the user once the program knn_tutorial.py
                                    runs. Will show the car condition, prediction and the data involved for the car information


v17 Scikit-learn / Un-organized Info: Info using scikit is able to be seen by user, however as of now
                                      the display is dis-organzied. Will work on fixing it
                                      Link for more info for scikit: https://scikit-learn.org/stable/


v18 SVM Tutorial : 'SVM stands for a support vector machine. SVM's are typically used for classification tasks similar
                        to what we did with K Nearest Neighbors. They work very well for high dimensional data and
                        are allow for us to classify data that does not have a linear correspondence.'
                        ~ Tim

                        New file -> svm_tutorial.py & source file -> svm_source.
                        Changed Introduction to KNN to Introduction to MLA.
                        Main objective for the file svm_tutorial is to see the breast cancer data set and check if the
                        classification can see if its tumor/cancer/non-cancer .

                        Edit: https://scikit-learn.org/stable/


v19 Learning On How Support Vector Machine Works: ' support vector machine works by dividing data into multiple classes
                                                    using something called a hyper-plane. A hyper plane is a fancy word
                                                    for something that is straight that can divide data points. In 2D space
                                                     a hyper-plane is simply a line, in 3D space a hyper-plane is a plane.
                                                      In any space higher than 3D it is simply called a hyper-plane.'
                                                      ~ Tim
                                                      Link: https://techwithtim.net/tutorials/machine-learning-python/svm-2/
                                                          The larger the distance / easier to seperate the points and classify
                                                      it at a better prediction on what it is.

                                                     Kernels:
                                                     'Kernels provide a way for us to create a hyper-plane for data like seen above.
                                                      We use a kernel to bring our data up to a higher dimension (in this case from 2D->3D).
                                                      We hope that by doing this we will have our points plotted in a way that we
                                                      can divide them using a hyper-plane.'

                                                      ~ Tim
                                                      Function for kernel f(x1,x2) -> x3 or f(x1,x2) = x3


----------------------------    ----------------------------   ------------------------- -----------------------------
v20 KNeighborsClassifier: line 21 -> clf = KNeighborsClassifier(n_neighbors=15) used to find nearest neighbors

v21 Unsupervised Algorithm K Means Clustering: Aquired K Means Clustring example from Scikit ->
                                                   Intro To K Means Clustering/scikit_example_kmeans.py

                                                   Link to Tim's quote on K-Means
                                                   https://techwithtim.net/tutorials/machine-learning-python/k-means-1/
                                                   K-Means Clustering
'K Means clustering is an unsupervised learning algorithm that attempts to divide our training data into k unique clusters to
classify information. This means this algorithm does not require labels for given test data. It is responsible for learning the
differences between our data points and determine what features determining what class.

Supervised vs Unsupervised Algorithm
Up until this point we have been using supervised machine learning algorithms. Supervised learning means that when we
pass training data to our algorithm we also pass the estimated values or classes for each of those data points. For
example, when we were classifying the safety of cars we gave the algorithm the features of the car and we told it if
the car was safe or not. If we were using an unsupervised algorithm we would only pass the features and omit the class.

How K-Means Clustering Works
The K-Means clustering algorithm is a classification algorithm that follows the steps outlined below to
cluster data points together. It attempts to separate each area of our high dimensional space into sections that
represent each class. When we are using it to predict it will simply find what section our point is in and assign it to
 that class.

Step 1: Randomly pick K points to place K centroids
Step 2: Assign all of the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.
Step 3: Average all of the points belonging to each centroid to find the middle of those clusters (center of mass).
Place the corresponding centroids into that position.
Step 4: Reassign every point once again to the closest centroid.
Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.'
----------------------------    ----------------------------   ------------------------- -----------------------------
--v22 K Means: New file created k_means.py with source folder containing both scikit example & k means source info.
               link for more info -> https://techwithtim.net/tutorials/machine-learning-python/k-means-2/
                 https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
                 https://scikit-learn.org/stable/modules/clustering.html
                 https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation


